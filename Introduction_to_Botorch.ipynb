{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Introduction_to_Botorch.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_wEfJ_FcN9c",
        "colab_type": "text"
      },
      "source": [
        "# BoTorch Tutorials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu2gzuRGcN9d",
        "colab_type": "text"
      },
      "source": [
        "The following information is summarised from the main BoTorch website.\n",
        "https://botorch.org/docs/introduction.html\n",
        "\n",
        "I have decided to summarise it to extract the core information from the documents to get started.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzWdvPY7cN9e",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to BoTorch\n",
        "\n",
        "BoTorch is a library for Bayesian Optimisation research built on top of PyTorch. \n",
        "\n",
        "Bayesian Optimisation (BayesOpt) is an established technique for sequential optimisation of costly-to-evaluate black box functions. It can be applied to a wide variety of problems, including hyperparameter optimisation for machine learning algorithms, A/B testing, as well as many scientific and engineering problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj26AxhOcN9f",
        "colab_type": "text"
      },
      "source": [
        "## Why Botorch?\n",
        "\n",
        "1. Improved Developer Efficiency\n",
        "Modular and easily extensible interface for composing \n",
        "\n",
        "*   Bayesian Optimisation primitives (include probabilitic mdels, acquisition functions and optimisers)\n",
        "*   Utilise quasi-Monte-Carlo acquisition functions\n",
        "\n",
        "\n",
        "2. State-of-the-art Modelling\n",
        "*   Provide support for state-of-the-art probabilisitc models in GPyTorch (Library for efficient, scalable Gaussian Process implemented in PyTorch).\n",
        "*   Features include multi-task GPs, deep kernel learning, deep GPs, and approximate inference\n",
        "\n",
        "\n",
        "3. Harnessing the features of PyTorch\n",
        "*   Auto-differentiation, GPU implementations and dynamic computation graph "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DRNZGUccN9f",
        "colab_type": "text"
      },
      "source": [
        "# Simple Programme to get started"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "O0VRs_g9cN9g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "a2d47e87-8aa7-406c-9657-814664a0864f"
      },
      "source": [
        "# ===============\n",
        "# Install BoTorch\n",
        "# ===============\n",
        "\n",
        "# Via conda\n",
        "# conda install botorch -c pytorch -c gpytorch\n",
        "\n",
        "# Via pip\n",
        "!pip install botorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting botorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/e4/d696b12a84d505e9592fb6f8458a968b19efc22e30cc517dd2d2817e27e4/botorch-0.2.1-py3-none-any.whl (221kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 6.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 81kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 92kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 215kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from botorch) (1.4.1)\n",
            "Collecting gpytorch>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c7/0c31802b84fc55aa069943c844eaccb0e420e91d7f4ed07cc5e1d127c458/gpytorch-1.1.1.tar.gz (240kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 22.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 13.3MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 17.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 17.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92kB 17.2MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 14.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from botorch) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->botorch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.1->botorch) (0.16.0)\n",
            "Building wheels for collected packages: gpytorch\n",
            "  Building wheel for gpytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpytorch: filename=gpytorch-1.1.1-py2.py3-none-any.whl size=400467 sha256=69c31a85387f48ab60cd0be642d3a40b7eef8ed5f2eb0b505c0825f9d31fdc9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/a5/29/4dafc0624adf678108e0067836556f0c72588e85d851d78ae0\n",
            "Successfully built gpytorch\n",
            "Installing collected packages: gpytorch, botorch\n",
            "Successfully installed botorch-0.2.1 gpytorch-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FMupZBePcN9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "76d5ae99-6fd9-4320-827f-b1a8735fde45"
      },
      "source": [
        "# ===============\n",
        "# Fitting a Model\n",
        "# ===============\n",
        "\n",
        "import torch\n",
        "from botorch.models import SingleTaskGP\n",
        "from botorch.fit import fit_gpytorch_model\n",
        "from botorch.utils import standardize\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "\n",
        "\n",
        "# Create training set\n",
        "train_X = torch.rand(10,2)  # Create random matrix 10 rows x 2 columns\n",
        "Y = 1-torch.norm(train_X,dim=-1,keepdim=True) # Generate Y = 1-sqrt(X1^2+X2^2)\n",
        "\n",
        "# randn_like(.)\n",
        "# Returns a tensor with the same size as input that is filled with \n",
        "# random numbers from a normal distribution with mean 0 and variance 1\n",
        "Y = Y + 0.1*torch.randn_like(Y)  # Add some noise (Size of Y)\n",
        "\n",
        "# standardize(.)\n",
        "# Standardizes (zero mean, unit variance) a tensor by dim=-2.\n",
        "# If the tensor is single-dimensional, simply standardizes the tensor. \n",
        "# If for some batch index all elements are equal (of if there is only a single \n",
        "# data point), this function will return 0 for that batch index.\n",
        "train_Y = standardize(Y) # Standardisation of Y for training\n",
        "\n",
        "# Gaussian Process Regression models based on GPyTorch models.\n",
        "# SingleTaskGP(train_X, train_Y, likelihood=None, covar_module=None, outcome_transform=None)\n",
        "# Single-task exact GP model\n",
        "# A single-task exact GP using relatively strong priors on the Kernel hyperparameters, \n",
        "# which work best when covariates are normalized to the unit cube and outcomes are standardized \n",
        "# (zero mean, unit variance).\n",
        "# This model works in batch mode (each batch having its own hyperparameters). When the \n",
        "# training observations include multiple outputs, this model will use batching to model outputs \n",
        "# independently.\n",
        "# Use this model when you have independent output(s) and all outputs use the same training data. \n",
        "# If outputs are independent and outputs have different training data, use the ModelListGP. \n",
        "# When modeling correlations between outputs, use the MultiTaskGP.\n",
        "# Parameters\n",
        "# train_X (Tensor) – A batch_shape x n x d tensor of training features.\n",
        "# train_Y (Tensor) – A batch_shape x n x m tensor of training observations.\n",
        "# likelihood (Optional[Likelihood]) – A likelihood. \n",
        "#       If omitted, use a standard GaussianLikelihood with inferred noise level.\n",
        "# covar_module (Optional[Module]) – The module computing the covariance (Kernel) matrix. \n",
        "#       If omitted, use a MaternKernel.\n",
        "# outcome_transform (Optional[OutcomeTransform]) – An outcome transform that is applied \n",
        "#       to the training data during instantiation and to the posterior during inference \n",
        "#       (that is, the Posterior obtained by calling .posterior on the model will be on \n",
        "#       the original scale).\n",
        "gp = SingleTaskGP(train_X,train_Y)\n",
        "\n",
        "# These are modules to compute (or approximate/bound) the marginal log likelihood (MLL) \n",
        "# of the GP model when applied to data.\n",
        "# These models are typically used as the “loss” functions for GP models\n",
        "# The exact marginal log likelihood (MLL) for an exact Gaussian process with a Gaussian likelihood.\n",
        "mll = ExactMarginalLogLikelihood(gp.likelihood,gp)\n",
        "\n",
        "\n",
        "# fit_gpytorch_model(mll, optimizer=<function fit_gpytorch_scipy>, **kwargs)\n",
        "# Fit hyperparameters of a GPyTorch model.\n",
        "# On optimizer failures, a new initial condition is sampled from the hyperparameter \n",
        "# priors and optimization is retried. The maximum number of retries can be passed \n",
        "# in as a max_retries kwarg (default is 5).\n",
        "# Returns MarginalLogLikelihood with optimized parameters. \n",
        "# For GP models, we are picking the best kernel for the GP.\n",
        "fit_gpytorch_model(mll)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExactMarginalLogLikelihood(\n",
              "  (likelihood): GaussianLikelihood(\n",
              "    (noise_covar): HomoskedasticNoise(\n",
              "      (noise_prior): GammaPrior()\n",
              "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
              "    )\n",
              "  )\n",
              "  (model): SingleTaskGP(\n",
              "    (likelihood): GaussianLikelihood(\n",
              "      (noise_covar): HomoskedasticNoise(\n",
              "        (noise_prior): GammaPrior()\n",
              "        (raw_noise_constraint): GreaterThan(1.000E-04)\n",
              "      )\n",
              "    )\n",
              "    (mean_module): ConstantMean()\n",
              "    (covar_module): ScaleKernel(\n",
              "      (base_kernel): MaternKernel(\n",
              "        (lengthscale_prior): GammaPrior()\n",
              "        (raw_lengthscale_constraint): Positive()\n",
              "        (distance_module): Distance()\n",
              "      )\n",
              "      (outputscale_prior): GammaPrior()\n",
              "      (raw_outputscale_constraint): Positive()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "W-iR3Z6LcN9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ==================================\n",
        "# Constructing acquisition function\n",
        "# ==================================\n",
        "\n",
        "from botorch.acquisition import UpperConfidenceBound\n",
        "UCB = UpperConfidenceBound(gp,beta=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nkXp5tXf_3N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0eb9ceb-714a-404b-df35-776369e73b10"
      },
      "source": [
        "# =================================\n",
        "# Optimise the acquisition function\n",
        "# =================================\n",
        "\n",
        "from botorch.optim import optimize_acqf\n",
        "\n",
        "bounds = torch.stack([torch.zeros(2),torch.ones(2)])\n",
        "\n",
        "\n",
        "# optimize_acqf(acq_function, bounds, q, num_restarts, raw_samples, options=None, \n",
        "#               inequality_constraints=None, equality_constraints=None, fixed_features=None, \n",
        "#               post_processing_func=None, batch_initial_conditions=None, return_best_only=True, \n",
        "#               sequential=False)\n",
        "# \n",
        "# Generate a set of candidates via multi-start optimization.\n",
        "#\n",
        "# ==========\n",
        "# Parameters\n",
        "# ==========\n",
        "#\n",
        "# acq_function (AcquisitionFunction) : An Acquisition Function.\n",
        "# bounds (Tensor) : A 2 x d tensor of lower and upper bounds for each column of X.\n",
        "# q (int) : The number of candidates.\n",
        "# num_restarts (int) : The number of starting points for multistart acquisition function optimization.\n",
        "# raw_samples (int) : The number of samples for initialization.\n",
        "# options (Optional[Dict[str, Union[bool, float, int, str]]]) : Options for candidate generation.\n",
        "# constraints (equality) : A list of tuples (indices, coefficients, rhs), with each tuple \n",
        "#                          encoding an inequality constraint of the form sum_i \n",
        "#                          (X[indices[i]] * coefficients[i]) >= rhs\n",
        "# constraints : A list of tuples (indices, coefficients, rhs), with each tuple encoding an \n",
        "#               inequality constraint of the form sum_i (X[indices[i]] * coefficients[i]) = rhs\n",
        "# fixed_features (Optional[Dict[int, float]]) : A map {feature_index: value} for features that should \n",
        "#                                               be fixed to a particular value during generation.\n",
        "# post_processing_func (Optional[Callable[[Tensor], Tensor]]) : A function that post-processes an \n",
        "#                                                               optimization result appropriately \n",
        "#                                                  (i.e., according to round-trip transformations).\n",
        "# batch_initial_conditions (Optional[Tensor]) : A tensor to specify the initial conditions. \n",
        "#                                               Set this if you do not want to use default \n",
        "#                                               initialization strategy.\n",
        "# return_best_only (bool) : If False, outputs the solutions corresponding to all random restart \n",
        "#                           initializations of the optimization.\n",
        "# sequential (bool) : If False, uses joint optimization, otherwise uses sequential optimization.\n",
        "#\n",
        "# =======\n",
        "# Returns\n",
        "# =======\n",
        "# 1. (num_restarts) x q x d-dim tensor of generated candidates\n",
        "# 2. tensor of associated acquisiton values\n",
        "#    If sequential=False, this is a (num_restarts)-dim tensor of joint acquisition values \n",
        "#    (with explicit restart dimension if return_best_only=False).\n",
        "#    If sequential=True, this is a q-dim tensor of expected acquisition values conditional \n",
        "#    on having observed canidates 0,1,…,i-1.\n",
        "candidate,acq_value = optimize_acqf(UCB,bounds=bounds,q=1,num_restarts=5,raw_samples=20,)\n",
        "candidate"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2485, 0.2005]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIRQNpenlrCZ",
        "colab_type": "text"
      },
      "source": [
        "## Constrained BO with qEI and qNEI\n",
        "\n",
        "In this section, we will use the batch Expected Improvement (qEI) and batch Noist Expected Improvement (qNEI) acquisition functions to optimize a constrained version of the synthetic Hartmann test function.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I3aL_vcs0Xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.double"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}